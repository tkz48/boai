use llm_client::clients::openai_compatible::OpenAICompatibleClient;
use llm_client::clients::types::LLMClient;
use llm_client::provider::OpenAICompatibleConfig;
use llm_client::{clients::types::LLMClientCompletionStringRequest, provider::LLMProviderAPIKeys};

#[tokio::main]
async fn main() {
    let api_key = LLMProviderAPIKeys::OpenAICompatible(OpenAICompatibleConfig {
        api_key: "some_key".to_owned(),
        api_base: "https://g6k8w0va7tp4p6-42424.proxy.runpod.net/v1".to_owned(),
    });
    let client = OpenAICompatibleClient::new();
    let prompt = "<｜fim▁begin｜>/// /Users/skcd/scratch/sidecar/llm_client/src/clients/openai_compatible.rs\npub fn new() -> Self {\n        Self {}\n    }\n\n    pub fn model(&self, model: &LLMType) -> Option<String> {\n        match model {\n            LLMType::GPT3_5_16k => Some(\"gpt-3.5-turbo-16k-0613\".to_owned()),\n            LLMType::Gpt4 => Some(\"gpt-4-0613\".to_owned()),\n            LLMType::Gpt4Turbo => Some(\"gpt-4-1106-preview\".to_owned()),\n            LLMType::Gpt4_32k => Some(\"gpt-4-32k-0613\".to_owned()),\n            LLMType::DeepSeekCoder33BInstruct => Some(\"deepseek-coder-33b\".to_owned()),\n            LLMType::DeepSeekCoder6BInstruct => Some(\"deepseek-coder-6b\".to_owned()),\n            _ => None,\n        }\n    }\n\n    pub fn messages(\n        &self,\n        messages: &[LLMClientMessage],\n    ) -> Result<Vec<ChatCompletionRequestMessage>, LLMClientError> {\n        let formatted_messages = messages\n            .into_iter()\n            .map(|message| {\n                let role = message.role();\n                match role {\n                    LLMClientRole::User => ChatCompletionRequestMessageArgs::default()\n                        .role(Role::User)\n                        .content(message.content().to_owned())\n                        .build()\n                        .map_err(|e| LLMClientError::OpenAPIError(e)),\n                    LLMClientRole::System => ChatCompletionRequestMessageArgs::default()\n                        .role(Role::System)\n                        .content(message.content().to_owned())\n                        .build()\n                        .map_err(|e| LLMClientError::OpenAPIError(e)),\n                    // the assistant is the one which ends up calling the function, so we need to\n                    // handle the case where the function is called by the assistant here\n                    LLMClientRole::Assistant => match message.get_function_call() {\n                        Some(function_call) => ChatCompletionRequestMessageArgs::default()\n                            .role(Role::Function)\n                            .function_call(FunctionCall {\n                                name: function_call.name().to_owned(),\n                                arguments: function_call.arguments().to_owned(),\n                            })\n                            .build()\n                            .map_err(|e| LLMClientError::OpenAPIError(e)),\n                        None => ChatCompletionRequestMessageArgs::default()\n                            .role(Role::Assistant)\n                            .content(message.content().to_owned())\n                            .build()\n                            .map_err(|e| LLMClientError::OpenAPIError(e)),\n                    },\n                    LLMClientRole::Function => match message.get_function_call() {\n                        Some(function_call) => ChatCompletionRequestMessageArgs::default()\n                            .role(Role::Function)\n                            .content(message.content().to_owned())\n                            .function_call(FunctionCall {\n                                name: function_call.name().to_owned(),\n                                arguments: function_call.arguments().to_owned(),\n                            })\n                            .build()\n                            .map_err(|e| LLMClientError::OpenAPIError(e)),\n                        None => Err(LLMClientError::FunctionCallNotPresent),\n                    },\n                }\n            })\n            .collect::<Vec<_>>();\n        formatted_messages\n            .into_iter()\n            .collect::<Result<Vec<ChatCompletionRequestMessage>, LLMClientError>>()\n    }\n\n    fn generate_openai_client(\n        &self,\n        api_key: LLMProviderAPIKeys,\n        llm_model: &LLMType,\n    ) -> Result<OpenAIClientType, LLMClientError> {\n        if <｜fim▁hole｜>        match api_key {\n            LLMProviderAPIKeys::OpenAICompatible(openai_compatible) => {\n                let config = OpenAIConfig::new()\n                    .with_api_key(openai_compatible.api_key)\n                    .with_api_base(openai_compatible.api_base);\n                Ok(OpenAIClientType::OpenAIClient(Client::with_config(config)))\n            }\n            _ => Err(LLMClientError::WrongAPIKeyType),\n        }\n    }\n\n    fn generate_completion_openai_client(\n        &self,\n        api_key: LLMProviderAPIKeys,\n        llm_model: &LLMType,\n    ) -> Result<Client<OpenAIConfig>, LLMClientError> {\n        match api_key {\n            LLMProviderAPIKeys::OpenAICompatible(openai_compatible) => {\n                let config = OpenAIConfig::new()\n                    .with_api_key(openai_compatible.api_key)\n                    .with_api_base(openai_compatible.api_base);\n                Ok(Client::with_config(config))\n            }\n            _ => Err(LLMClientError::WrongAPIKeyType),\n        }\n    }\n}\n\n#[async_trait]\nimpl LLMClient for OpenAICompatibleClient {\n    fn client(&self) -> &crate::provider::LLMProvider {\n        &crate::provider::LLMProvider::OpenAICompatible\n    }\n\n    async fn stream_completion(\n        &self,\n        api_key: LLMProviderAPIKeys,\n        request: LLMClientCompletionRequest,\n        sender: tokio::sync::mpsc::UnboundedSender<LLMClientCompletionResponse>,\n    ) -> Result<String, LLMClientError> {\n        let llm_model = request.model();\n        let model = self.model(llm_model);\n        if model.is_none() {\n            return Err(LLMClientError::UnSupportedModel);\n        }\n        let model = model.unwrap();\n        let messages = self.messages(request.messages())?;\n        let mut request_builder_args = CreateChatCompletionRequestArgs::default();\n        let mut request_builder = request_builder_args\n            .model(model.to_owned())\n            .messages(messages)\n            .temperature(request.temperature())\n            .stream(true);\n        if let Some(frequency_penalty) = request.frequency_penalty() {\n            request_builder = request_builder.frequency_penalty(frequency_penalty);\n        }\n        let request = request_builder.build()?;\n        let mut buffer = String::new();\n        let client = self.generate_openai_client(api_key, llm_model)?;\n\n        // TODO(skcd): Bad code :| we are repeating too many things but this\n        // just works and we need it right now\n        match client {\n            OpenAIClientType::AzureClient(client) => {\n                let stream_maybe = client.chat().create_stream(request).await;\n                if stream_maybe.is_err() {\n                    return Err(LLMClientError::OpenAPIError(stream_maybe.err().unwrap()));\n                } else {\n                    dbg!(\"no error here\");\n                }\n                let mut stream = stream_maybe.unwrap();\n                while let Some(response) = stream.next().await {\n                    match response {\n                        Ok(response) => {\n                            let delta = response\n                                .choices\n                                .get(0)\n                                .map(|choice| choice.delta.content.to_owned())\n                                .flatten()\n                                .unwrap_or(\"\".to_owned());\n                            let _value = response\n                                .choices\n                                .get(0)\n                                .map(|choice| choice.delta.content.as_ref())\n                                .flatten();\n                            buffer.push_str(&delta);\n                            let _ = sender.send(LLMClientCompletionResponse::new(\n                                buffer.to_owned(),\n                                Some(delta),\n                                model.to_owned(),\n                            ));\n                        }\n                        Err(err) => {\n                            dbg!(err);\n                            break;\n                        }\n                    }\n                }\n            }\n            OpenAIClientType::OpenAIClient(client) => {<｜fim▁end｜>";
    let (sender, _receiver) = tokio::sync::mpsc::unbounded_channel();
    let request = LLMClientCompletionStringRequest::new(
        llm_client::clients::types::LLMType::DeepSeekCoder33BInstruct,
        prompt.to_owned(),
        0.2,
        None,
    )
    .set_max_tokens(100);
    let response = client
        .stream_prompt_completion(api_key, request, sender)
        .await;
    println!("{}", response.expect("to work"));
}
